---
title: "What is Worth Replicating? - SIPS 2020 Unconference session"
author: "Peder M. Isager & Anna van ‘t Veer"
date: '2020-06-30'
output:
  pdf_document: default
  html_document: null
header:
  caption: ''
  image: headers/SIPS_2020_header.png
slug: what-is-worth-replicating
tags:
- replication value
- replication
- SIPS
- unconference
categories: []
---



<p>Given that most research is original and we have limited resources available for replication, we need guidelines for study selection in replication research. But what makes a study worth replicating? In our unconference session at SIPS this year, we had the pleasure of discussing this problem with fifty-some enthusiastic scholars. Here is a quick summary of what they taught us.</p>
<p>First off, we went into this session with our own prior beliefs about what makes a study worth replicating. Namely, we strongly believe that studies are worth replicating (1) if their results are uncertain, (2) if we can reduce uncertainty about the results by replicating, and (3) the results are worth becoming less uncertain about.</p>
<p>In part, this session was an opportunity for us to see if our beliefs are shared by others. In part, it gave us an opportunity to fish for blind spots in our own beliefs; perhaps there are important factors for determining what is worth replicating that we have not thought of?</p>
<p>To get the discussion going, we asked participants to focus on one of three questions:</p>
<ol style="list-style-type: decimal">
<li>Which specific studies/claims in your field of interest do you believe currently are the most in need of replication? Why these?</li>
<li>What criteria (general or specific to your field of interest) would you use to determine which studies in your field to spend resources replicating?</li>
<li>Have you ever conducted a replication? If so, how did you go about selecting a study to replicate? Did you need to select one of multiple candidates? If so, how did you choose among them?</li>
</ol>
<div id="factors-mentioned-that-jive-with-our-prior-beliefs" class="section level2">
<h2>Factors mentioned that jive with our prior beliefs:</h2>
<p>The discussion was mainly focused on general factors that contribute to determine replication worthiness (question 2). From the notes and summaries provided by the groups at the end of the session, we find that many of the factors that people brought up are related to the framework we had in mind going into the session. I.e. the uncertainty about existing knowledge, the value of having knowledge, and the possibility of gaining knowledge through replication, all seem to be factors that people brought up as important. It is also abundantly clear from the discussion that these factors are complicated, and cannot easily be tied down to any specific set of operationalizations. Here are just some of the factors that people brought up as potentially important for considering if something is worth replicating (<em>italicized indented bullet points indicate our post-session replies to comments</em>):</p>
<div id="uncertainty-about-existing-knowledge" class="section level3">
<h3>Uncertainty about existing knowledge</h3>
<ul>
<li><p>Distrust in how the original study was run</p></li>
<li><p>Statistical uncertainty (e.g. as measured by confidence intervals)</p></li>
<li><p>Distrust in the researcher/lab that conducted the study</p></li>
<li><p>Generalizability of results</p></li>
<li><p>Replicability</p></li>
<li><p>Small sample size/low statistical power</p></li>
<li><p>Suspicious data/prevalence of p-hacking</p></li>
<li><p>Results only replicable in a single lab</p></li>
<li><p>Methodological details missing from the report</p></li>
<li><p>“Fragile” findings</p></li>
<li><p>HARKed conclusions</p></li>
<li><p>Multiple explanations for the finding exist</p>
<ul>
<li><em>Reply: Here I think it is worth considering if a replication can actually help us mitigate the problem. If a study design leaves open multiple explanations for the results, and we simply replicate the design with more data, we may still not be able to separate between different explanations when we see our results. Thus, this kind of uncertainty might actually call for novel study designs rather than replication (e.g., in an adversarial collaboration).</em></li>
</ul></li>
<li><p>Degree of available evidence supporting the phenomenon</p></li>
<li><p>Certainty in result</p></li>
</ul>
</div>
<div id="our-ability-to-reduce-uncertainty" class="section level3">
<h3>Our ability to reduce uncertainty</h3>
<ul>
<li><p>Feasibility</p></li>
<li><p>Expense/Resource cost</p></li>
<li><p>Availability of materials</p></li>
<li><p>Replication estimates the efficacy and generalisability of specific stimuli</p></li>
<li><p>Replication estimates the degree to which an effect exists ‘in the wild’</p></li>
<li><p>Availability of stimuli, resources, full explanation of methods, etc.</p>
<ul>
<li><em>Reply: Although most replicators will of course strive for a rigorous replication, some replication studies might not add much in terms of reducing uncertainty (e.g. if no resources are available to achieve adequate power). It seems most of these bullets assume the replication itself is ‘good’ but we should stay critical there as well. For selecting a study to replicate, the question of how much uncertainty the replication can reduce may for instance be raised when phase 1 reviewers judge a replication proposal.</em></li>
</ul></li>
</ul>
</div>
<div id="value-of-becoming-certain" class="section level3">
<h3>Value of becoming certain</h3>
<ul>
<li><p>Scientific impact</p></li>
<li><p>“Real world” consequences</p></li>
<li><p>Number of citations</p>
<ul>
<li><em>Reply: However, as many pointed out, citation count is not a perfect indicator of impact, and some citations have little to do with the impact of the paper on a field. </em></li>
</ul></li>
<li><p>Impactfulness</p></li>
<li><p>Interesting study</p></li>
<li><p>Study design one intends to extend</p></li>
<li><p>Societal impact. (e.g. on public consumption, policy, interest/awareness/attention in media)</p></li>
<li><p>Number of scientists working on the topic? To avoid lots of wasted resources.</p></li>
<li><p>Number of people who could benefit from the knowledge.</p></li>
<li><p>International impact.</p></li>
<li><p>Potential benefit/harm for vulnerable populations.</p></li>
<li><p>Relevance (personal/theoretical/political)</p></li>
</ul>
<p>There was also a group that discussed study selection strategies in past replication research (question 3). The criteria mentioned by this group as having been important for actual study selection decisions largely overlapped with the factors listed above.</p>
</div>
</div>
<div id="factors-mentioned-that-do-not-obviously-jive-with-our-prior-beliefs." class="section level2">
<h2>Factors mentioned that do not obviously jive with our prior beliefs.</h2>
<p>It is always useful to understand when a normative (what should be replicated) framework does not coincide with descriptive reality (what is replicated), or when different normative frameworks collide. Here are a few examples of factors people brought up, that do not obviously fit within our three-category system for deciding what is worth replicating:</p>
<ul>
<li><p>Focus on replicating old studies vs new studies</p>
<ul>
<li><em>Reply: It is perhaps reasonable to assume that age of the study is correlated with the importance of the study for a field of research. Older studies have been in circulation for longer, and have had more time to become embedded in the research canon. However, some study designs and results become outdated with age (e.g. through improved study designs, or by having conclusions falsified by subsequent research). Thus, you could also argue that more recent studies are more likely to be influencing current theorizing and should be prioritized for replication. Both are perhaps equally sound conclusions in different scenarios, which would entail that there is no straight-forward relationship between age and replication worthiness of a study. </em></li>
</ul></li>
<li><p>Finding is surprising/violates common sense</p>
<ul>
<li><em>Reply: It is not completely clear to me whether this factor is mentioned because it entails uncertainty, because findings that clash with our intuitions are more valuable, or for some other reason. I agree that it is likely important, and certainly has been the motivation for several actual replication efforts. It would be interesting to dig further into why/when a surprising finding ought to be more worth replicating than an unsurprising one. </em></li>
</ul></li>
<li><p>Surveying the ‘health’ of a discipline as a whole</p>
<ul>
<li><em>Reply: A good example of a replication project with this goal in mind is the Reproducibility Project Psychology. In this project, even though authors were still motivated by things like impact and feasibility (see their reported study inclusion criteria), it is clear that their motivation was partly to get a representative sample of findings from the field. In this case, the desire to identify studies worth replicating must naturally be weighed against the need for unbiased sampling from the population of original studies. Thus, whether a study - in isolation - is worth replicating is not always the guiding principle for study selection.</em></li>
</ul></li>
<li><p>Arbitrariness (i.e. picking a random study)</p>
<ul>
<li><em>Reply: This goal makes sense in scenarios like the one described in the previous point. When the goal is to estimate average replicability in a field, random/arbitrary selection is necessary to ensure valid inferences about replicability to the population of psychology research. It is an interesting factor since it essentially prevents you from selecting on any other criteria. For a broader discussion of the benefits of random study selection, see Kuehberger, &amp; Schulte-Mecklenbeck, (2018). </em></li>
</ul></li>
<li><p>Teaching/pedagogy</p>
<ul>
<li><em>Reply: Several people brought this up during discussion. It nicely illustrates a point that one discussion group put a lot of emphasis on: the goal of the replication study will partially determine what is worth replicating. I.e. you cannot adequately answer what should be replicated without first defining what you hope the replication will achieve. For example, if our goal is to reduce uncertainty, then we likely want to replicate original studies that are highly uncertain. However, if our goal is pedagogical, we might want to replicate studies with highly certain outcomes (e.g. to demonstrate a principle, or to know when a study design was successfully implemented), even if replicating them does not reduce our uncertainty much.</em></li>
</ul></li>
<li><p>Some findings are reliable enough to be used as manipulations in later experiments</p>
<ul>
<li><em>Reply: This reminds me of replications that are done because the authors are motivated, not to replicate a finding, but to change or extend a study procedure (this was also mentioned by several people in our discussion). It is important to note that in cases where replication is just a minor step in reaching a larger goal, we may not be replicating to reduce our uncertainty about the results of prior research. In fact, we may actually prefer the results we are replicating to already be highly replicable (e.g. when we reuse a previously validated measurement tool). </em></li>
</ul></li>
</ul>
</div>
<div id="other-interesting-comments-suggestions-and-questions-and-our-replies." class="section level2">
<h2>Other interesting comments, suggestions, and questions (and our replies).</h2>
<ul>
<li><p>When do we say “there is no need of further replication” of a study or theoretical construct?</p>
<ul>
<li><p><em>Peder: This is a good and difficult question, and it relates to a few other comments made in the session. In general, you probably always need to consider this question as relative to other replication studies you could do. I.e. once you have conducted a replication of study A, is it worth replicating A again, or is there now a study B that has become more important to replicate than A, given that A is corroborated/falsified by the replication you just conducted? </em></p></li>
<li><p><em>Anna: this also reminds me that we as researchers are generally bad at knowing how to weigh the evidence we are looking at, perhaps because we are not used to calibrating our conclusions with the evidence, or because we are still catching up when it comes to reporting and expressing our certainty in terms of confidence intervals. For a good read on planning your sample size on the basis of how accurate you want your estimate to be, <a href="10.1146/annurev.psych.59.103006.093735">see Maxwell, Kelley &amp; Rausch, 2008</a>.</em></p></li>
</ul></li>
<li><p>Most replications are probably conducted by students interested in the subject matter at hand (which might incidentally include some of these factors like influence, importance, likelihood etc.).</p>
<ul>
<li><em>Reply: An important point. An implicit assumption behind our work is that researchers often have multiple replication candidates that would be equally interesting to replicate. Alternatively, you could perhaps assume that personal interest is not as important as replicating research with societal impact, highly uncertain results, etc., so personal interest should not factor into the equation. But these assumptions could of course be false, and it is certainly the case that many researchers select studies based on personal interest (see e.g. <a href="https://pedermisager.netlify.app/post/what-to-replicate/" class="uri">https://pedermisager.netlify.app/post/what-to-replicate/</a>).</em></li>
</ul></li>
<li><p>Retrospective registered report: If you get a paper results blind, would the method and rationale convince you that you want to run a replication?</p>
<ul>
<li><em>Reply: A very intriguing idea! To offer one caveat, I do think that results are sometimes important for knowing if a study is worth replicating. For example, if it turns out that the results of a study are extremely accurate because of very low variance in the estimates then one might want to conclude that this study does not need replication. However, one would need the results to know whether this is the case. On the other hand, one could perhaps assume that variance is mainly determined by factors such as sample size, which are known before seeing the results. </em></li>
</ul></li>
<li><p>Should we really buy into the idea of ‘not everything can be replicated’? If a research question was worth asking, worth funding, worth using participants/animals/resources to answer–isn’t it worth building in replication from the start? That is, instead of selecting completed projects to replicate, shouldn’t we design projects with internal and/or cross-lab replications from the start?</p>
<ul>
<li><em>Reply: Of course we should! The question is, whether we still want existing literature to factor into what we study. If we do, we need to know its robustness before we implement build-in replications/validations.</em></li>
</ul></li>
</ul>
</div>
<div id="links-to-materials." class="section level2">
<h2>Links to materials.</h2>
<p>PDF versions of all documents and notes pertaining to our session can be found on this OSF project page: <a href="https://osf.io/ufea4/" class="uri">https://osf.io/ufea4/</a>. If you’d like a more detailed introduction to our, the session hosts’, beliefs about what is worth replicating, I recommend checking out the youtube recording in a previous post: <a href="https://pedermisager.netlify.app/post/choosing-what-to-replicate-talk/" class="uri">https://pedermisager.netlify.app/post/choosing-what-to-replicate-talk/</a>.</p>
<p>We want to thank everyone involved in our unconference session for their time and contributions!</p>
</div>
